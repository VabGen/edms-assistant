# src/edms_assistant/rag/hybrid_search.py
import logging
from typing import List, Tuple, Dict, Any
from pathlib import Path
import pickle
import os

import numpy as np
from sklearn.preprocessing import MinMaxScaler
from rank_bm25 import BM25Okapi
from langchain_core.documents import Document
from langchain_community.vectorstores import FAISS

from edms_assistant.core.settings import settings

logger = logging.getLogger(__name__)


class HybridSearch:
    def __init__(self, vector_store: FAISS, chunks: List[Document]):
        self.vector_store = vector_store
        self.chunks = chunks

        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º BM25
        tokenized_corpus = [
            self._tokenize(doc.page_content) for doc in chunks
        ]
        self.bm25 = BM25Okapi(tokenized_corpus)
        logger.info(f"‚úÖ BM25 –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è {len(chunks)} —á–∞–Ω–∫–æ–≤")

    def _tokenize(self, text: str) -> List[str]:
        """–ü—Ä–æ—Å—Ç–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞"""
        # –£–¥–∞–ª—è–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é –∏ —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–ª–æ–≤–∞
        import re
        text = re.sub(r"[^\w\s]", " ", text.lower())
        return text.split()

    def search(
            self,
            query: str,
            k: int = 5,
            semantic_weight: float = 0.6,
            keyword_weight: float = 0.4
    ) -> List[Tuple[Document, float]]:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ (–¥–æ–∫—É–º–µ–Ω—Ç, –≥–∏–±—Ä–∏–¥–Ω—ã–π_—Å–∫–æ—Ä), –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ —É–±—ã–≤–∞–Ω–∏—é —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏.
        """
        # 1. Semantic search (FAISS)
        semantic_results = self.vector_store.similarity_search_with_score(query, k=k * 2)
        # FAISS –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç L2 distance ‚Üí –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ similarity: sim = 1 / (1 + dist)
        semantic_scores = {}
        for doc, dist in semantic_results:
            sim = 1.0 / (1.0 + dist)
            semantic_scores[id(doc)] = sim

        # 2. Keyword search (BM25)
        tokenized_query = self._tokenize(query)
        bm25_scores = self.bm25.get_scores(tokenized_query)

        # 3. –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º
        combined = []
        for i, doc in enumerate(self.chunks):
            sem_score = semantic_scores.get(id(doc), 0.0)
            kw_score = bm25_scores[i]

            hybrid_score = semantic_weight * sem_score + keyword_weight * kw_score
            combined.append((doc, hybrid_score))

        combined.sort(key=lambda x: x[1], reverse=True)

        logger.debug(f"üîç –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –∑–∞–ø—Ä–æ—Å—É: {query}")
        for i, (doc, score) in enumerate(combined[:3]):
            logger.debug(f"  –¢–æ–ø-{i + 1} (score={score:.3f}): {doc.page_content[:80]}...")
        return combined[:k]