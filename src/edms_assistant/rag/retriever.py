# src/edms_assistant/rag/retriever.py
import logging
from typing import List, Dict, Any
from pathlib import Path
import pickle

from langchain_openai import ChatOpenAI
from edms_assistant.core.settings import settings
from src.edms_assistant.rag.hybrid_search import HybridSearch

logger = logging.getLogger(__name__)


async def _expand_and_route_query(question: str, chat_history: List[Dict]) -> str:
    """–û–±—ä–µ–¥–∏–Ω—è–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç."""
    history = " ".join(
        msg["content"] for msg in chat_history[-2:] if msg["role"] == "user"
    ) if chat_history else ""

    input_text = f"–ö–æ–Ω—Ç–µ–∫—Å—Ç: {history}\n–í–æ–ø—Ä–æ—Å: {question}" if history else question

    llm = ChatOpenAI(
        api_key="not-needed",
        base_url=str(settings.vllm.generative_base_url),
        model=settings.vllm.generative_model,
        temperature=0.0
    )

    prompt = f"""–†–∞—Å—à–∏—Ä—å –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö, –¥–æ–±–∞–≤–∏–≤ —Å–∏–Ω–æ–Ω–∏–º—ã –∏ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã.
–í–æ–ø—Ä–æ—Å: {input_text}
–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å:"""

    resp = await llm.ainvoke([("user", prompt)])
    return resp.content.strip()


async def retrieve_and_generate(
        question: str,
        filename: str,
        chat_history: List[Dict[str, Any]],
        vector_store
) -> str:
    logger.debug(f"üí¨ –ò—Å—Ç–æ—Ä–∏—è —á–∞—Ç–∞ (–∏–∑ Redis): {chat_history}")
    # –§–æ—Ä–º–∏—Ä—É–µ–º enriched query
    enriched_query = await _expand_and_route_query(question, chat_history)
    logger.debug(f"üîç –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å: {enriched_query}")

    # –ó–∞–≥—Ä—É–∑–∫–∞ —á–∞–Ω–∫–æ–≤
    store_dir = Path(settings.paths.vector_stores_dir) / Path(filename).stem
    chunks_path = store_dir / "chunks.pkl"

    if not chunks_path.exists():
        logger.error(f"‚ùå –ß–∞–Ω–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã: {chunks_path}")
        return "REFLECT: –ù–µ –Ω–∞–π–¥–µ–Ω–æ –≤ —ç—Ç–æ–º —Ñ–∞–π–ª–µ"

    with open(chunks_path, "rb") as f:
        chunks = pickle.load(f)

    # –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫
    hybrid = HybridSearch(vector_store, chunks)
    results = hybrid.search(query=enriched_query, k=5)
    relevant_docs = [doc for doc, _ in results[:3]] if results else []

    if not relevant_docs:
        return "REFLECT: –ù–µ –Ω–∞–π–¥–µ–Ω–æ –≤ —ç—Ç–æ–º —Ñ–∞–π–ª–µ"

    context = "\n\n".join(
        f"[{doc.metadata.get('source', 'Unknown')}]\n{doc.page_content}"
        for doc in relevant_docs
    )
    logger.debug(f"üìö –ö–æ–Ω—Ç–µ–∫—Å—Ç:\n{context}")

    # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç —Å –∏—Å—Ç–æ—Ä–∏–µ–π
    history_str = "\n".join(
        f"{m['role']}: {m['content']}" for m in chat_history
    ) if chat_history else "–ù–µ—Ç –∏—Å—Ç–æ—Ä–∏–∏."

    system = f"""–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º. –û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –ø–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É.
–í–µ—Ä–Ω–∏ –ü–û–õ–ù–´–ô –æ—Ç–≤–µ—Ç —Å–æ –í–°–ï–ú–ò –¥–µ—Ç–∞–ª—è–º–∏. –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ—Ç ‚Äî —Å–∫–∞–∂–∏: ¬´–Ø –Ω–µ –Ω–∞—à—ë–ª –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö¬ª.

–ö–æ–Ω—Ç–µ–∫—Å—Ç:
{context}"""

    user = f"""–ò—Å—Ç–æ—Ä–∏—è:
{history_str}

–í–æ–ø—Ä–æ—Å: {question}"""

    llm = ChatOpenAI(
        api_key="not-needed",
        base_url=str(settings.vllm.generative_base_url),
        model=settings.vllm.generative_model,
        temperature=0.0,
        max_tokens=1024
    )

    resp = await llm.ainvoke([("system", system), ("user", user)])
    answer = resp.content.strip()

    if any(p in answer.lower() for p in ["–Ω–µ –Ω–∞—à—ë–ª", "–Ω–µ –º–æ–≥—É –Ω–∞–π—Ç–∏", "–Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"]):
        return "REFLECT: –ù–µ –Ω–∞–π–¥–µ–Ω–æ –≤ —ç—Ç–æ–º —Ñ–∞–π–ª–µ"

    return answer
