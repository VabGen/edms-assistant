import asyncio
import logging
import os
import hashlib
import json
from typing import Literal, List

from langgraph.prebuilt import tools_condition
from pydantic import BaseModel, Field
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langgraph.graph import StateGraph, START, END
from langchain_core.messages import HumanMessage, AIMessage, ToolMessage

# === –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è ===

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(name)s: %(message)s",
    handlers=[
        logging.StreamHandler()  # –í—ã–≤–æ–¥ –≤ –∫–æ–Ω—Å–æ–ª—å
    ]
)

logger = logging.getLogger(__name__)

# === Pydantic-—Å—Ö–µ–º—ã ===

class ModelConfig(BaseModel):
    generative_base_url: str = "http://model-generative.shared.du.iba/v1"
    generative_model: str = "generative-model"
    embedding_base_url: str = "http://model-embedding.shared.du.iba/v1"
    embedding_model: str = "Qwen/Qwen3-Embedding-8B"
    api_key: str = "not-needed"

# === –ö–ª–∏–µ–Ω—Ç –¥–ª—è –≤—ã–∑–æ–≤–∞ —Ç–≤–æ–∏—Ö –º–æ–¥–µ–ª–µ–π (—Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º langchain_openai) ===

class ModelClient:
    def __init__(self, config: ModelConfig):
        self.config = config

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM
        self.llm = ChatOpenAI(
            api_key=self.config.api_key,
            base_url=self.config.generative_base_url,
            model=self.config.generative_model,
            temperature=0.0,
        )

        # –ü–æ–ø—ã—Ç–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å Embeddings
        try:
            self.embeddings = OpenAIEmbeddings(
                api_key=self.config.api_key,
                base_url=self.config.embedding_base_url,
                model=self.config.embedding_model,  # ‚Üê –ò–°–ü–û–õ–¨–ó–£–ï–¢–°–Ø –ü–†–ê–í–ò–õ–¨–ù–û–ï –ò–ú–Ø –ú–û–î–ï–õ–ò: "Qwen/Qwen3-Embedding-8B"
            )
            logger.success("‚úÖ Embeddings —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã.")
            self.embeddings_available = True
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ embeddings: {e}")
            logger.info("üí° Embeddings –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã. –ë—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω –∫–∞—Å—Ç–æ–º–Ω—ã–π –ø–æ–∏—Å–∫.")
            self.embeddings = None
            self.embeddings_available = False

    async def generate_text(self, prompt: str, system_prompt: str = None) -> str:
        messages = []
        if system_prompt:
            messages.append(("system", system_prompt))
        messages.append(("user", prompt))

        logger.info(f"üîÑ [LLM] –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–π –º–æ–¥–µ–ª–∏: {self.config.generative_base_url}")
        response = await self.llm.ainvoke(messages)
        logger.success("‚úÖ [LLM] –û—Ç–≤–µ—Ç –æ—Ç –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–π –º–æ–¥–µ–ª–∏ –ø–æ–ª—É—á–µ–Ω.")
        return response.content

    async def embed_text(self, texts: list[str]) -> list[list[float]]:
        if not self.embeddings_available:
            raise RuntimeError("‚ùå Embeddings –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã. –°–µ—Ä–≤–µ—Ä –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç /v1/embeddings.")
        logger.info(f"üîÑ [EMBED] –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤–æ–π –º–æ–¥–µ–ª–∏: {self.config.embedding_base_url}")
        # OpenAIEmbeddings.embed_documents –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –≤–µ–∫—Ç–æ—Ä–æ–≤
        result = await self.embeddings.aembed_documents(texts)
        logger.success("‚úÖ [EMBED] –û—Ç–≤–µ—Ç –æ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –ø–æ–ª—É—á–µ–Ω.")
        return result

# === NLP-–∫–æ–º–ø–æ–Ω–µ–Ω—Ç (—Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º LLM) ===

class NLUProcessor:
    def __init__(self, model_client: ModelClient):
        self.model_client = model_client

    async def classify_intent(self, text: str) -> str:
        """Classify the intent of the user's text."""
        logger.info(f"üîÑ [NLU] –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –Ω–∞–º–µ—Ä–µ–Ω–∏—è –¥–ª—è —Ç–µ–∫—Å—Ç–∞: {text[:50]}...")
        prompt = f"""
<role>
–¢—ã ‚Äî –ø–æ–º–æ—â–Ω–∏–∫ —Å–∏—Å—Ç–µ–º—ã —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å –Ω–∞–º–µ—Ä–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.
</role>

<task>
–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ç–µ–∫—Å—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏ –æ–ø—Ä–µ–¥–µ–ª–∏ –µ–≥–æ –Ω–∞–º–µ—Ä–µ–Ω–∏–µ.
</task>

<available_intents>
- find_instruction: –∑–∞–ø—Ä–æ—Å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∏–ª–∏ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∫–∞–∫ —á—Ç–æ-—Ç–æ —Å–¥–µ–ª–∞—Ç—å)
- create_document: —Å–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
- find_document: –ø–æ–∏—Å–∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
- update_document: –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
- delete_document: —É–¥–∞–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
- find_employee: –ø–æ–∏—Å–∫ —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∞
- unknown: –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–µ –Ω–∞–º–µ—Ä–µ–Ω–∏–µ
</available_intents>

<text>
{text}
</text>

<thought>
–ü–æ–¥—É–º–∞–π, –∫–∞–∫–æ–µ –Ω–∞–º–µ—Ä–µ–Ω–∏–µ –ø–æ–¥—Ö–æ–¥–∏—Ç.
</thought>

<output>
–û—Ç–≤–µ—Ç—å —Ç–æ–ª—å–∫–æ –æ–¥–Ω–∏–º —Å–ª–æ–≤–æ–º: "find_instruction", "create_document", "find_document", "update_document", "delete_document", "find_employee" –∏–ª–∏ "unknown".
</output>
"""

        system_prompt = "–¢—ã –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –Ω–∞–º–µ—Ä–µ–Ω–∏–π. –û—Ç–≤–µ—á–∞–π —Ç–æ–ª—å–∫–æ –æ–¥–Ω–∏–º —Å–ª–æ–≤–æ–º."
        response = await self.model_client.generate_text(prompt, system_prompt=system_prompt)
        intent = response.strip().lower()
        logger.info(f"‚úÖ [NLU] intent = {intent}")
        return intent

    async def extract_entities(self, text: str) -> dict:
        """Extract entities from the user's text."""
        logger.info(f"üîÑ [NLU] –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π –∏–∑ —Ç–µ–∫—Å—Ç–∞: {text[:50]}...")
        prompt = f"""
<role>
–¢—ã ‚Äî –ø–æ–º–æ—â–Ω–∏–∫ —Å–∏—Å—Ç–µ–º—ã —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –∏–∑–≤–ª–µ–∫–∞—Ç—å —Å—É—â–Ω–æ—Å—Ç–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.
</role>

<task>
–ò–∑–≤–ª–µ–∫–∏ —Å–ª–µ–¥—É—é—â–∏–µ —Å—É—â–Ω–æ—Å—Ç–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞:
- employee_name: –∏–º—è —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∞
- document_id: ID –¥–æ–∫—É–º–µ–Ω—Ç–∞
- reg_number: —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π –Ω–æ–º–µ—Ä
- date: –¥–∞—Ç–∞ (–≤ –ª—é–±–æ–º —Ñ–æ—Ä–º–∞—Ç–µ)
- uuid: UUID (–Ω–∞–ø—Ä–∏–º–µ—Ä, 550e8400-e29b-41d4-a716-446655440000)
</task>

<text>
{text}
</text>

<thought>
–ü–æ–¥—É–º–∞–π, –∫–∞–∫–∏–µ —Å—É—â–Ω–æ—Å—Ç–∏ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç.
</thought>

<output>
–û—Ç–≤–µ—Ç—å –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON: {{"employee_name": [], "document_id": [], "reg_number": [], "date": [], "uuid": []}}
</output>
"""

        system_prompt = "–¢—ã –∏–∑–≤–ª–µ–∫–∞—Ç–µ–ª—å —Å—É—â–Ω–æ—Å—Ç–µ–π. –û—Ç–≤–µ—á–∞–π —Ç–æ–ª—å–∫–æ JSON."
        response = await self.model_client.generate_text(prompt, system_prompt=system_prompt)
        import json
        try:
            entities = json.loads(response.strip())
        except json.JSONDecodeError:
            logger.error("‚ùå [NLU] –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON –∏–∑ NLU.")
            entities = {"employee_name": [], "document_id": [], "reg_number": [], "date": [], "uuid": []}
        logger.info(f"‚úÖ [NLU] entities = {entities}")
        return entities

    async def preprocess_query(self, query: str) -> str:
        """Preprocess the user's query."""
        logger.info(f"üîÑ [NLU] –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤–æ–ø—Ä–æ—Å–∞: {query[:50]}...")
        prompt = f"""
<role>
–¢—ã ‚Äî –ø–æ–º–æ—â–Ω–∏–∫ —Å–∏—Å—Ç–µ–º—ã —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî —É–ª—É—á—à–∏—Ç—å –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.
</role>

<task>
–ü–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π –≤–æ–ø—Ä–æ—Å, —á—Ç–æ–±—ã –æ–Ω –±—ã–ª –±–æ–ª–µ–µ —Ç–æ—á–Ω—ã–º –∏ –ø–æ–Ω—è—Ç–Ω—ã–º.
</task>

<question>
{query}
</question>

<thought>
–ü–æ–¥—É–º–∞–π, –∫–∞–∫ –º–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å –≤–æ–ø—Ä–æ—Å.
</thought>

<output>
–í–µ—Ä–Ω–∏ —É–ª—É—á—à–µ–Ω–Ω—ã–π –≤–æ–ø—Ä–æ—Å.
</output>
"""

        response = await self.model_client.generate_text(prompt)
        improved_query = response.strip()
        logger.info(f"‚úÖ [NLU] improved_query = {improved_query}")
        return improved_query

# === –ü—Ä–æ—Å—Ç–æ–π RAG-–∫–æ–º–ø–æ–Ω–µ–Ω—Ç (–Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ–∏—Å–∫–∞ –ø–æ —Ñ–∞–π–ª—É) ===

class SimpleRAG:
    def __init__(self, model_client: ModelClient):
        self.model_client = model_client
        self.chunks: list[str] = []
        self.embeddings: list[list[float]] = []

    def _get_file_id(self, file_path: str) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç ID —Ñ–∞–π–ª–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –µ–≥–æ –∏–º–µ–Ω–∏ –∏ –ø—É—Ç–∏."""
        file_hash = hashlib.md5(file_path.encode()).hexdigest()[:8]
        return f"{os.path.basename(file_path)}_{file_hash}"

    async def load_and_chunk_file(self, file_path: str):
        logger.info(f"üîÑ [RAG] –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞: {file_path}")
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"‚ùå [RAG] –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")

        # –ü—Ä–æ—Å—Ç–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ
        if file_path.endswith('.doc'):
            logger.info("üîÑ [RAG] –§–∞–π–ª .doc –æ–±–Ω–∞—Ä—É–∂–µ–Ω. –ü–æ–ø—ã—Ç–∫–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ pypandoc...")
            try:
                import pypandoc
                text = pypandoc.convert_file(file_path, 'plain', format='doc')
            except ImportError:
                logger.error("‚ùå [RAG] pypandoc –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install pypandoc")
                logger.error("üí° [RAG] –ò–ª–∏ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–π—Ç–µ —Ñ–∞–π–ª –≤ .docx/.txt –≤—Ä—É—á–Ω—É—é.")
                return
            except Exception as e:
                logger.error(f"‚ùå [RAG] –û—à–∏–±–∫–∞ –ø—Ä–∏ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ .doc: {e}")
                return
        elif file_path.endswith('.docx'):
            from docx import Document
            doc = Document(file_path)
            text = '\n'.join([p.text for p in doc.paragraphs if p.text])
        elif file_path.endswith('.txt'):
            with open(file_path, 'r', encoding='utf-8') as f:
                text = f.read()
        else:
            raise ValueError(f"‚ùå [RAG] –ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞: {file_path}")

        # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏ (–∫—É—Å–æ—á–∫–∏)
        chunk_size = 500
        self.chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]
        logger.info(f"‚úÖ [RAG] –§–∞–π–ª —Ä–∞–∑–±–∏—Ç –Ω–∞ {len(self.chunks)} —á–∞–Ω–∫–æ–≤.")

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —á–∞–Ω–∫–æ–≤, –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã
        if self.model_client.embeddings_available:
            logger.info("üîÑ [RAG] –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è —á–∞–Ω–∫–æ–≤...")
            try:
                self.embeddings = await self.model_client.embed_text(self.chunks)
                logger.success("‚úÖ [RAG] –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã.")

                # --- –°–û–•–†–ê–ù–ï–ù–ò–ï FAISS –í –ü–ê–ü–ö–£ cli ---
                file_id = self._get_file_id(file_path)
                faiss_dir = f"./cli/faiss_index_{file_id}"
                os.makedirs(faiss_dir, exist_ok=True)

                from langchain_community.vectorstores import FAISS
                vector_store = FAISS.from_embeddings(
                    text_embeddings=list(zip(self.chunks, self.embeddings)),
                    embedding=self.model_client.embeddings
                )
                vector_store.save_local(faiss_dir)
                logger.success(f"‚úÖ [RAG] FAISS-–∏–Ω–¥–µ–∫—Å —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤: {faiss_dir}")

            except RuntimeError as e:
                logger.error(f"‚ùå [RAG] –û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {e}")
                logger.info("üí° [RAG] –û—Ç–∫–∞—Ç –∫ –∫–∞—Å—Ç–æ–º–Ω–æ–º—É –ø–æ–∏—Å–∫—É.")
                self.embeddings = []
                self.model_client.embeddings_available = False
        else:
            logger.info("üí° [RAG] Embeddings –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã. –ß–∞–Ω–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã.")
            # --- –°–û–•–†–ê–ù–ï–ù–ò–ï –ß–ê–ù–ö–û–í –í JSON –í –ü–ê–ü–ö–£ cli ---
            file_id = self._get_file_id(file_path)
            chunks_file = f"./cli/chunks_{file_id}.json"
            os.makedirs("./cli", exist_ok=True)

            with open(chunks_file, 'w', encoding='utf-8') as f:
                json.dump(self.chunks, f, ensure_ascii=False, indent=2)
            logger.success(f"‚úÖ [RAG] –ß–∞–Ω–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {chunks_file}")

    def cosine_similarity(self, a: list[float], b: list[float]) -> float:
        dot_product = sum(i * j for i, j in zip(a, b))
        magnitude_a = sum(i ** 2 for i in a) ** 0.5
        magnitude_b = sum(i ** 2 for i in b) ** 0.5
        if magnitude_a == 0 or magnitude_b == 0:
            return 0.0
        return dot_product / (magnitude_a * magnitude_b)

    async def query(self, question: str) -> str:
        logger.info(f"üîÑ [RAG] –ü–æ–ª—É—á–µ–Ω –≤–æ–ø—Ä–æ—Å: {question}")

        if self.model_client.embeddings_available and self.embeddings:
            # --- –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ ---
            logger.info("üîç [RAG] –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫.")
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –≤–æ–ø—Ä–æ—Å–∞
            question_embedding = (await self.model_client.embed_text([question]))[0]

            # –ù–∞—Ö–æ–¥–∏–º –±–ª–∏–∂–∞–π—à–∏–π —á–∞–Ω–∫
            similarities = [self.cosine_similarity(question_embedding, emb) for emb in self.embeddings]
            best_idx = max(range(len(similarities)), key=lambda i: similarities[i])
            best_chunk = self.chunks[best_idx]

            logger.info(f"‚úÖ [RAG] –õ—É—á—à–∏–π —á–∞–Ω–∫ –Ω–∞–π–¥–µ–Ω (—Å—Ö–æ–∂–µ—Å—Ç—å: {similarities[best_idx]:.3f})")
            return best_chunk
        else:
            # --- –ö–∞—Å—Ç–æ–º–Ω—ã–π –ø–æ–∏—Å–∫ (–ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º) ---
            logger.info("üîç [RAG] –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–∞—Å—Ç–æ–º–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º.")
            question_words = set(question.lower().split())

            best_chunk = ""
            best_score = 0

            for chunk in self.chunks:
                chunk_lower = chunk.lower()
                score = sum(1 for word in question_words if word in chunk_lower)

                if score > best_score:
                    best_score = score
                    best_chunk = chunk

            if best_chunk:
                logger.info(f"‚úÖ [RAG] –ù–∞–π–¥–µ–Ω –ª—É—á—à–∏–π —á–∞–Ω–∫ —Å {best_score} —Å–æ–≤–ø–∞–¥–∞—é—â–∏–º–∏ —Å–ª–æ–≤–∞–º–∏.")
            else:
                logger.info("‚ùå [RAG] –ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ–¥—Ö–æ–¥—è—â–µ–≥–æ —á–∞–Ω–∫–∞.")
                best_chunk = "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ."

            return best_chunk

# === –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è RAG (–∞–Ω–∞–ª–æ–≥ LangChain `create_retriever_tool`) ===

class RAGTool:
    def __init__(self, rag: SimpleRAG):
        self.rag = rag
        self.name = "retrieve_edms_manual"
        self.description = "Search and return information about EDMS manual."

    async def invoke(self, query: str) -> str:
        logger.info(f"üîÑ [TOOL] RAGTool.invoke –≤—ã–∑–≤–∞–Ω —Å –∑–∞–ø—Ä–æ—Å–æ–º: {query[:50]}...")
        result = await self.rag.query(query)
        logger.success(f"‚úÖ [TOOL] RAGTool.invoke –≤–µ—Ä–Ω—É–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª–∏–Ω–æ–π {len(result)} —Å–∏–º–≤–æ–ª–æ–≤.")
        return result

# === –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è (–∞–Ω–∞–ª–æ–≥ MessagesState) ===

class AgentState(BaseModel):
    messages: List[dict] = Field(default_factory=list)
    intent: str = None  # ‚Üê –ù–û–í–û–ï –ü–û–õ–ï: –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –Ω–∞–º–µ—Ä–µ–Ω–∏—è
    entities: dict = Field(default_factory=dict)  # ‚Üê –ù–û–í–û–ï –ü–û–õ–ï: –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Å—É—â–Ω–æ—Å—Ç–µ–π
    decision: str = None  # ‚Üê –ù–û–í–û–ï –ü–û–õ–ï: –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ—à–µ–Ω–∏—è –∏–∑ grade_documents

# === –£–∑–ª—ã –∞–≥–µ–Ω—Ç–∞ (—Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –ø—Ä–æ–º–ø—Ç–∞–º–∏ –∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º) ===

class AgenticRAG:
    def __init__(self, model_client: ModelClient, rag_tool: RAGTool, nlu_processor: NLUProcessor):
        self.model_client = model_client
        self.rag_tool = rag_tool
        self.nlu_processor = nlu_processor

    async def preprocess_query_node(self, state: AgentState):
        """Preprocess the user's query using NLU."""
        logger.info("üîÑ [NODE] preprocess_query_node –∑–∞–ø—É—â–µ–Ω.")
        question = state.messages[-1]["content"]
        logger.debug(f"üìù [NODE] –í—Ö–æ–¥–Ω–æ–π –≤–æ–ø—Ä–æ—Å: {question}")
        improved_question = await self.nlu_processor.preprocess_query(question)
        # –û–±–Ω–æ–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
        state.messages[-1]["content"] = improved_question
        # –¢–∞–∫–∂–µ –∏–∑–≤–ª–µ–∫–∞–µ–º intent –∏ entities
        intent = await self.nlu_processor.classify_intent(improved_question)
        entities = await self.nlu_processor.extract_entities(improved_question)
        logger.info(f"‚úÖ [NODE] preprocess_query_node –∑–∞–≤–µ—Ä—à—ë–Ω. Intent: {intent}, Entities: {entities}")
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º dict, –∫–æ—Ç–æ—Ä—ã–π –æ–±–Ω–æ–≤–ª—è–µ—Ç —Å–æ—Å—Ç–æ—è–Ω–∏–µ
        return {"intent": intent, "entities": entities}

    async def generate_query_or_respond(self, state: AgentState):
        """Call the model to generate a response based on the current state. Given
        the question, it will decide to retrieve using the retriever tool, or simply respond to the user.
        """
        logger.info(f"üîÑ [NODE] generate_query_or_respond –∑–∞–ø—É—â–µ–Ω. Intent: {state.intent}")
        last_message = state.messages[-1]
        question = last_message["content"]
        intent = state.intent

        # --- –ö–õ–Æ–ß–ï–í–û–ï –ò–ó–ú–ï–ù–ï–ù–ò–ï: –Ø–í–ù–û –í–´–ó–´–í–ê–ï–ú –ò–ù–°–¢–†–£–ú–ï–ù–¢, –ï–°–õ–ò –ù–ê–ú–ï–†–ï–ù–ò–ï –ü–û–î–•–û–î–ò–¢ ---
        if intent == "find_instruction":
            logger.info("üîç [NODE] Intent 'find_instruction' -> –≤—ã–∑–æ–≤ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞.")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ —Å –≤—ã–∑–æ–≤–æ–º –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞
            # –í–ê–ñ–ù–û: tool_calls –î–û–õ–ñ–ï–ù –ë–´–¢–¨ –í –°–û–û–ë–©–ï–ù–ò–ò, –ö–û–¢–û–†–û–ï –í–û–ó–í–†–ê–©–ê–ï–¢–°–Ø –ö–ê–ö AIMessage
            return {"messages": [AIMessage(
                content="",
                tool_calls=[
                    {"name": self.rag_tool.name, "args": {"query": question}, "id": "call_1"}
                ]
            ).dict()]}

        else:
            logger.info("üí¨ [NODE] Intent –Ω–µ 'find_instruction' -> –ø—Ä—è–º–æ–π –æ—Ç–≤–µ—Ç.")
            response_content = await self.model_client.generate_text(question,
                                                                     system_prompt="–¢—ã –ø–æ–º–æ—â–Ω–∏–∫ –ø–æ –°–≠–î. –û—Ç–≤–µ—Ç—å –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ –¥–µ–ª—É.")
            return {"messages": [AIMessage(content=response_content).dict()]}

    async def grade_documents(self, state: AgentState) -> Literal["generate_answer", "rewrite_question"]:
        """Determine whether the retrieved documents are relevant to the question."""
        logger.info("üîÑ [NODE] grade_documents –∑–∞–ø—É—â–µ–Ω.")
        question = state.messages[0]["content"]
        # –ü—Ä–µ–¥–ø–æ–ª–æ–∂–∏–º, —á—Ç–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç ‚Äî —ç—Ç–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ ToolMessage
        tool_response = state.messages[-1]["content"]  # –≠—Ç–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç RAG

        grade_prompt = f"""
<role>
–¢—ã ‚Äî –æ—Ü–µ–Ω—â–∏–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞.
</role>

<task>
–û–ø—Ä–µ–¥–µ–ª–∏, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ –∏–∑–≤–ª–µ—á—ë–Ω–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –æ—Ç–Ω–æ—Å—è—â—É—é—Å—è –∫ –≤–æ–ø—Ä–æ—Å—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.
</task>

<question>
{question}
</question>

<thought>
–ü–æ–¥—É–º–∞–π, —Ä–µ–ª–µ–≤–∞–Ω—Ç–µ–Ω –ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç.
</thought>

<output>
–û—Ç–≤–µ—Ç—å 'yes', –µ—Å–ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–µ–Ω, –∏ 'no', –µ—Å–ª–∏ –Ω–µ—Ç.
</output>
"""

        grade_system_prompt = "–¢—ã –æ—Ü–µ–Ω—â–∏–∫. –û—Ç–≤–µ—á–∞–π —Ç–æ–ª—å–∫–æ 'yes' –∏–ª–∏ 'no'."
        grade_response = await self.model_client.generate_text(grade_prompt, system_prompt=grade_system_prompt)

        score = grade_response.strip().lower()
        decision = "generate_answer" if score == "yes" else "rewrite_question"
        logger.info(f"‚úÖ [NODE] grade_documents –∑–∞–≤–µ—Ä—à—ë–Ω. –†–µ—à–µ–Ω–∏–µ: {decision}, Score: {score}")
        return decision

    async def rewrite_question(self, state: AgentState):
        """Rewrite the original user question."""
        logger.info("üîÑ [NODE] rewrite_question –∑–∞–ø—É—â–µ–Ω.")
        question = state.messages[0]["content"]

        rewrite_prompt = f"""
<role>
–¢—ã ‚Äî –ø–æ–º–æ—â–Ω–∏–∫, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –≤–æ–ø—Ä–æ—Å—ã.
</role>

<task>
–ü–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, —á—Ç–æ–±—ã –æ–Ω –±—ã–ª –±–æ–ª–µ–µ —Ç–æ—á–Ω—ã–º –∏ –ø–æ–Ω—è—Ç–Ω—ã–º.
</task>

<question>
{question}
</question>

<thought>
–ü–æ–¥—É–º–∞–π, –∫–∞–∫ –º–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å –≤–æ–ø—Ä–æ—Å.
</thought>

<output>
–í–µ—Ä–Ω–∏ —É–ª—É—á—à–µ–Ω–Ω—ã–π –≤–æ–ø—Ä–æ—Å.
</output>
"""

        rewrite_response = await self.model_client.generate_text(rewrite_prompt)
        new_question = rewrite_response.strip()
        logger.info(f"‚úÖ [NODE] rewrite_question –∑–∞–≤–µ—Ä—à—ë–Ω. –ù–æ–≤—ã–π –≤–æ–ø—Ä–æ—Å: {new_question}")
        return {"messages": [HumanMessage(content=new_question).dict()]}

    async def generate_answer(self, state: AgentState):
        """Generate an answer."""
        logger.info("üîÑ [NODE] generate_answer –∑–∞–ø—É—â–µ–Ω.")
        question = state.messages[0]["content"]
        # –ö–æ–Ω—Ç–µ–∫—Å—Ç ‚Äî –∏–∑ ToolMessage
        context = state.messages[-1]["content"]

        generate_prompt = f"""
<role>
–¢—ã ‚Äî –ø–æ–º–æ—â–Ω–∏–∫ –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –°–≠–î (–°–∏—Å—Ç–µ–º—ã –≠–ª–µ–∫—Ç—Ä–æ–Ω–Ω–æ–≥–æ –î–æ–∫—É–º–µ–Ω—Ç–æ–æ–±–æ—Ä–æ—Ç–∞).
</role>

<task>
–û—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, —Å—Ç—Ä–æ–≥–æ –æ–ø–∏—Ä–∞—è—Å—å –Ω–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç.
</task>

<constraints>
- –ï—Å–ª–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –æ—Ç–≤–µ—Ç–∞, —á–µ—Å—Ç–Ω–æ —Å–∫–∞–π: '–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ.'
- –ù–µ –≤—ã–¥—É–º—ã–≤–∞–π –∏ –Ω–µ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –∫–æ—Ç–æ—Ä–æ–π –Ω–µ—Ç –≤ —Ç–µ–∫—Å—Ç–µ.
</constraints>

<format>
–§–æ—Ä–º—É–ª–∏—Ä—É–π –æ—Ç–≤–µ—Ç –∫—Ä–∞—Ç–∫–æ, –≤ 1-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è—Ö, –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.
</format>

<question>
{question}
</question>

<context>
{context}
</context>
"""

        generate_response = await self.model_client.generate_text(generate_prompt)
        logger.success("‚úÖ [NODE] generate_answer –∑–∞–≤–µ—Ä—à—ë–Ω.")
        return {"messages": [AIMessage(content=generate_response).dict()]}

# === –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã–∑–æ–≤–∞ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞ (–∞–Ω–∞–ª–æ–≥ ToolNode) ===

async def call_tool_node(state: AgentState, rag_tool: RAGTool):
    logger.info("üîÑ [NODE] call_tool_node –∑–∞–ø—É—â–µ–Ω.")
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∞—Ä–≥—É–º–µ–Ω—Ç—ã –≤—ã–∑–æ–≤–∞ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞ –∏–∑ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è
    last_message = state.messages[-1]
    tool_calls = last_message.get("tool_calls", [])
    if not tool_calls:
        logger.info("üí° [NODE] call_tool_node: –Ω–µ—Ç tool_calls.")
        return state

    tool_call = tool_calls[0]
    if tool_call["name"] == rag_tool.name:
        query = tool_call["args"]["query"]
        logger.info(f"üîç [NODE] –í—ã–∑–æ–≤ RAGTool —Å –∑–∞–ø—Ä–æ—Å–æ–º: {query}")
        result = await rag_tool.invoke(query)
        tool_message = ToolMessage(content=result, name=rag_tool.name, tool_call_id=tool_call["id"])
        logger.success("‚úÖ [NODE] call_tool_node –∑–∞–≤–µ—Ä—à—ë–Ω.")
        return {"messages": [tool_message.dict()]}

# === –°–±–æ—Ä–∫–∞ –≥—Ä–∞—Ñ–∞ ===

async def build_graph(model_client: ModelClient, rag_tool: RAGTool, nlu_processor: NLUProcessor):
    logger.info("üîÑ [GRAPH] –°–æ–∑–¥–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∞...")
    agent = AgenticRAG(model_client, rag_tool, nlu_processor)

    workflow = StateGraph(AgentState)

    workflow.add_node("preprocess_query", agent.preprocess_query_node)
    workflow.add_node("generate_query_or_respond", agent.generate_query_or_respond)
    workflow.add_node("retrieve", lambda state: call_tool_node(state, rag_tool))

    # grade_documents —Ç–µ–ø–µ—Ä—å ‚Äî —É–∑–µ–ª, –∫–æ—Ç–æ—Ä—ã–π –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç dict —Å decision
    # –ò–°–ü–†–ê–í–õ–ï–ù–û: –æ–±–µ—Ä–Ω—É—Ç—å –≤—ã–∑–æ–≤ –≤ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é, –∫–æ—Ç–æ—Ä–∞—è –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç dict
    async def grade_documents_node(state: AgentState):
        result = await agent.grade_documents(state)
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º dict, –∫–æ—Ç–æ—Ä—ã–π –æ–±–Ω–æ–≤–ª—è–µ—Ç —Å–æ—Å—Ç–æ—è–Ω–∏–µ
        return {"decision": result}

    workflow.add_node("grade_documents", grade_documents_node)
    workflow.add_node("rewrite_question", agent.rewrite_question)
    workflow.add_node("generate_answer", agent.generate_answer)

    workflow.add_edge(START, "preprocess_query")
    workflow.add_edge("preprocess_query", "generate_query_or_respond")

    # –ò–°–ü–û–õ–¨–ó–£–ï–ú tools_condition (–≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π —É–∑–µ–ª LangGraph)
    # –û–Ω –ø—Ä–æ–≤–µ—Ä—è–µ—Ç, –µ—Å—Ç—å –ª–∏ tool_calls –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–º —Å–æ–æ–±—â–µ–Ω–∏–∏
    workflow.add_conditional_edges(
        "generate_query_or_respond",
        tools_condition,
        {
            "tools": "retrieve",  # ‚Üê –ï—Å–ª–∏ –µ—Å—Ç—å tool_calls -> –≤—ã–∑–≤–∞—Ç—å retrieve
            END: END,             # ‚Üê –ï—Å–ª–∏ –Ω–µ—Ç -> –∑–∞–∫–æ–Ω—á–∏—Ç—å
        },
    )

    workflow.add_edge("retrieve", "grade_documents")

    # –ò–°–ü–†–ê–í–õ–ï–ù–û: –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è –æ—Ü–µ–Ω–∫–∏
    def route_after_grade(state: AgentState):
        # state.decision ‚Äî —ç—Ç–æ —Å—Ç—Ä–æ–∫–∞, –∫–æ—Ç–æ—Ä—É—é –º—ã —Å–æ—Ö—Ä–∞–Ω–∏–ª–∏ –≤ grade_documents_node
        return state.decision

    workflow.add_conditional_edges(
        "grade_documents",
        route_after_grade,
        {
            "generate_answer": "generate_answer",
            "rewrite_question": "rewrite_question"
        }
    )

    workflow.add_edge("generate_answer", END)
    workflow.add_edge("rewrite_question", "generate_query_or_respond")

    logger.success("‚úÖ [GRAPH] –ì—Ä–∞—Ñ —Å–æ–∑–¥–∞–Ω.")
    return workflow.compile()

# === –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ç–µ—Å—Ç ===

async def interactive_demo():
    config = ModelConfig()
    model_client = ModelClient(config)

    # –°–æ–∑–¥–∞—ë–º NLU-–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä
    nlu = NLUProcessor(model_client)

    # –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞
    file_path = input("–í–≤–µ–¥–∏—Ç–µ –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É (–Ω–∞–ø—Ä–∏–º–µ—Ä, –†—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ_–ø–æ_EDMS.docx): ").strip()
    if not os.path.exists(file_path):
        print(f"‚ùå [CLI] –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")
        return

    rag = SimpleRAG(model_client)
    await rag.load_and_chunk_file(file_path)

    # –ü–†–û–í–ï–†–ò–¢–¨, –ó–ê–ì–†–£–ñ–ï–ù–´ –õ–ò –ß–ê–ù–ö–ò
    if not rag.chunks:
        print("‚ùå [CLI] –ß–∞–Ω–∫–∏ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã.")
        return

    # –ü–†–û–í–ï–†–ò–¢–¨, –ó–ê–ì–†–£–ñ–ï–ù–´ –õ–ò –≠–ú–ë–ï–î–î–ò–ù–ì–ò
    if model_client.embeddings_available and not rag.embeddings:
        print("‚ùå [CLI] –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –Ω–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã. –ü—Ä–æ–≤–µ—Ä—å URL —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤–æ–π –º–æ–¥–µ–ª–∏.")
        return

    rag_tool = RAGTool(rag)

    print("\n‚úÖ [CLI] –ê—Å—Å–∏—Å—Ç–µ–Ω—Ç –≥–æ—Ç–æ–≤. –ó–∞–¥–∞–≤–∞–π –≤–æ–ø—Ä–æ—Å—ã. –í–≤–µ–¥–∏ 'exit' –¥–ª—è –≤—ã—Ö–æ–¥–∞.\n")

    graph = await build_graph(model_client, rag_tool, nlu)

    while True:
        question = input("–í–æ–ø—Ä–æ—Å: ").strip()
        if question.lower() in ['exit', 'quit', '–≤—ã–π—Ç–∏', 'q']:
            print("üëã [CLI] –í—ã—Ö–æ–¥.")
            break

        if not question:
            continue

        try:
            logger.info(f"üîÑ [CLI] –ü–æ–ª—É—á–µ–Ω –≤–æ–ø—Ä–æ—Å: {question}")
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –Ω–∞—á–∞–ª—å–Ω–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è
            initial_state = AgentState(messages=[HumanMessage(content=question).dict()])

            # –ó–∞–ø—É—Å–∫ –≥—Ä–∞—Ñ–∞
            final_state = await graph.ainvoke(initial_state)

            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞
            final_message = final_state["messages"][-1]
            response = final_message.get("content", "‚ùå [CLI] –ù–µ—Ç –æ—Ç–≤–µ—Ç–∞")
            print(f"ü§ñ [CLI] –û—Ç–≤–µ—Ç: {response}\n")
        except Exception as e:
            print(f"‚ùå [CLI] –û—à–∏–±–∫–∞: {e}")

if __name__ == "__main__":
    asyncio.run(interactive_demo())