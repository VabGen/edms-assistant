import asyncio
import logging
import os
from typing import List, Tuple
import numpy as np
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import Docx2txtLoader
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from pydantic import BaseModel
import docx2txt

print(docx2txt.__file__)

# === –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è ===

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(name)s: %(message)s",
    handlers=[
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# === Pydantic-—Å—Ö–µ–º—ã ===

class ModelConfig(BaseModel):
    generative_base_url: str = "http://model-generative.shared.du.iba/v1"
    generative_model: str = "generative-model"
    embedding_base_url: str = "http://model-embedding.shared.du.iba/v1"
    embedding_model: str = "embedding-model"

# === –ö–ª–∏–µ–Ω—Ç –¥–ª—è –≤—ã–∑–æ–≤–∞ –º–æ–¥–µ–ª–µ–π ===

class ModelClient:
    def __init__(self, config: ModelConfig):
        self.config = config
        logger.info(f"üîß –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è embedding_model: {self.config.embedding_model}")
        logger.info(f"üîß –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è generative_model: {self.config.generative_model}")

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM
        self.llm = ChatOpenAI(
            api_key="not-needed",
            base_url=self.config.generative_base_url,
            model=self.config.generative_model,
            temperature=0.6,
        )

        try:
            self.embeddings = OpenAIEmbeddings(
                api_key="not-needed",
                base_url=self.config.embedding_base_url,
                model=self.config.embedding_model,
            )
            logger.info("‚úÖ Embeddings —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã.")
            self.embeddings_available = True
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ embeddings: {e}")
            logger.info("üí° Embeddings –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã. –ë—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω –∫–∞—Å—Ç–æ–º–Ω—ã–π –ø–æ–∏—Å–∫.")
            self.embeddings = None
            self.embeddings_available = False

    async def generate_text(self, prompt: str, system_prompt: str = None) -> str:
        messages = []
        if system_prompt:
            messages.append(("system", system_prompt))
        messages.append(("user", prompt))

        logger.info(f"üîÑ [LLM] –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–π –º–æ–¥–µ–ª–∏: {self.config.generative_base_url}")
        response = await self.llm.ainvoke(messages)
        logger.info("‚úÖ [LLM] –û—Ç–≤–µ—Ç –æ—Ç –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–π –º–æ–¥–µ–ª–∏ –ø–æ–ª—É—á–µ–Ω.")
        return response.content

    async def embed_text(self, texts: list[str]) -> list[list[float]]:
        if not self.embeddings_available:
            raise RuntimeError("‚ùå Embeddings –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã. –°–µ—Ä–≤–µ—Ä –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç /v1/embeddings.")
        logger.info(f"üîÑ [EMBED] –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤–æ–π –º–æ–¥–µ–ª–∏: {self.config.embedding_base_url}")
        result = await self.embeddings.aembed_documents(texts)
        logger.info("‚úÖ [EMBED] –û—Ç–≤–µ—Ç –æ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –ø–æ–ª—É—á–µ–Ω.")
        return result

# === –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ ===

KNOWLEDGE_BASE_FILE = r"D:\project\edms-assistant\cli\doc\–°–≠–î.docx"
VECTOR_STORE_DIR = r"D:\project\edms-assistant\cli\vector"

DOCS_IN_RETRIEVER = 15
RELEVANCE_THRESHOLD_DOCS = 0.5
RELEVANCE_THRESHOLD_PROMPT = 0.4

# === –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Vector Store ===

def save_vector_store(vector_store, vector_store_dir: str):
    vector_store.save_local(vector_store_dir)
    print(f"Vector store —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤: {vector_store_dir}")

def load_vector_store(vector_store_dir: str, embeddings):
    index_file = os.path.join(vector_store_dir, "index.faiss")
    if not os.path.exists(index_file):
        print(f"–§–∞–π–ª {index_file} –Ω–µ –Ω–∞–π–¥–µ–Ω. –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å vector store.")
        return None
    try:
        vector_store = FAISS.load_local(
            vector_store_dir,
            embeddings,
            allow_dangerous_deserialization=True
        )
        print(f"Vector store –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑: {vector_store_dir}")
        return vector_store
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ vector store: {e}")
        return None

# === –§—É–Ω–∫—Ü–∏—è –∑–∞–≥—Ä—É–∑–∫–∏ –∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ ===

import pickle

def save_docs_for_later(documents, filepath="D:\\project\\edms-assistant\\cli\\vector\\split_docs.pkl"):
    with open(filepath, "wb") as f:
        pickle.dump(documents, f)
    print(f"–ß–∞–Ω–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –¥–ª—è –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –ø–æ–ø—ã—Ç–∫–∏: {filepath}")

def load_docs_from_disk(filepath="D:\\project\\edms-assistant\\cli\\vector\\split_docs.pkl"):
    if os.path.exists(filepath):
        with open(filepath, "rb") as f:
            return pickle.load(f)
    return None

def load_and_index_documents(file_path: str, vector_store_dir: str, embeddings) -> bool:
    os.makedirs(vector_store_dir, exist_ok=True)

    vector_store = load_vector_store(vector_store_dir, embeddings)
    if vector_store:
        print("–°—É—â–µ—Å—Ç–≤—É—é—â–∏–π vector store —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω.")
        return True

    # –∑–∞–≥—Ä—É–∑–∏—Ç—å —É–∂–µ —Ä–∞–∑–¥–µ–ª—ë–Ω–Ω—ã–µ —á–∞–Ω–∫–∏ –∏–∑ –¥–∏—Å–∫–∞
    split_docs = load_docs_from_disk()
    if split_docs is None:
        documents = []
        if not os.path.exists(file_path):
            print(f"–§–∞–π–ª {file_path} –Ω–µ –Ω–∞–π–¥–µ–Ω.")
            return False

        if file_path.lower().endswith(".docx"):
            try:
                loader = Docx2txtLoader(file_path)
                doc_docs = loader.load()
                documents.extend(doc_docs)
                print(f"–î–æ–±–∞–≤–ª–µ–Ω–æ {len(doc_docs)} —Å—Ç—Ä–∞–Ω–∏—Ü –∏–∑ {os.path.basename(file_path)}")
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ {os.path.basename(file_path)}: {e}")
        else:
            print(f"–§–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞ {file_path} –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è.")
            return False

        if not documents:
            print("–ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–∏—è.")
            return False

        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        split_docs = text_splitter.split_documents(documents)
        # -------------
        for i, doc in enumerate(split_docs[:5]):
            print(f"\n--- –ß–∞–Ω–∫ {i + 1} ---")
            print(doc.page_content)
            print(f"–ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ: {doc.metadata}")
        print(f"–í—Å–µ–≥–æ –ø–æ–ª—É—á–µ–Ω–æ {len(split_docs)} —á–∞–Ω–∫–æ–≤ –ø–æ—Å–ª–µ —Ä–∞–∑–±–∏–µ–Ω–∏—è.")
        # ------------

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —á–∞–Ω–∫–∏, —á—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ—á–∏—Ç—ã–≤–∞—Ç—å DOCX –∫–∞–∂–¥—ã–π —Ä–∞–∑
        save_docs_for_later(split_docs)

    # === –ø—Ä–æ–±—É–µ–º —Å–æ–∑–¥–∞—Ç—å –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ ===
    batch_size = 2
    vector_store = None

    for i in range(0, len(split_docs), batch_size):
        batch = split_docs[i:i + batch_size]
        print(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞ {i // batch_size + 1}/{(len(split_docs) + batch_size - 1) // batch_size}...")

        try:
            if vector_store is None:
                vector_store = FAISS.from_documents(batch, embeddings)
            else:
                vector_store.add_documents(batch)
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –±–∞—Ç—á–∞ {i // batch_size + 1}: {str(e)}")
            print("üí° –ü–æ–¥–æ–∂–¥–∏, –ø–æ–∫–∞ embedding-—Å–µ—Ä–≤–µ—Ä –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è.")
            return False

    print("–î–æ–∫—É–º–µ–Ω—Ç—ã —É—Å–ø–µ—à–Ω–æ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω—ã –≤ FAISS.")
    save_vector_store(vector_store, vector_store_dir)
    return True

# === –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ ===

async def preprocess_user_prompt(user_prompt: str, chat_history: list, model_client: ModelClient) -> str:
    instructions = (
        "Your task is to refine the user prompt below, preserving its meaning.\n"
        "Steps to follow:\n"
        "1. Identify the main question or request.\n"
        "2. If there are multiple tasks, list them.\n"
        "3. Keep the text concise and clear.\n\n"
        f"User prompt:\n{user_prompt}\n\n"
        "Chat history:\n"
        f"{chat_history}\n"
        "-----\n"
        "Now, provide the improved prompt below:\n"
    )
    system_prompt = "You are an assistant. Refine the user's prompt."
    response = await model_client.generate_text(instructions, system_prompt=system_prompt)
    improved_prompt = response.strip()
    return improved_prompt

# === –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ ===

def retrieve_documents(
        vector_store,
        user_prompt: str,
        k: int = 20,
        metadata_filters: dict = None
):
    """
    –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–∏—Å–∫ –ø–æ –≤–µ–∫—Ç–æ—Ä–Ω–æ–º—É —Ö—Ä–∞–Ω–∏–ª–∏—â—É FAISS (similarity search).
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (Document, score).
    """
    if not vector_store:
        print("Vector store –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω. –°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –∏–Ω–¥–µ–∫—Å.")
        return []

    try:
        if metadata_filters:
            docs_with_scores = vector_store.similarity_search_with_score(
                user_prompt,
                k=k,
                filter=metadata_filters
            )
        else:
            docs_with_scores = vector_store.similarity_search_with_score(user_prompt, k=k)
        return docs_with_scores
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {e}")
        return []

# === –ù–ï –ù–£–ñ–ù–û! –£–¥–∞–ª—è–µ–º compute_embeddings_similarity_async ===
# (–û—Å—Ç–∞–≤–ª—è—é —Ç–æ–ª—å–∫–æ FAISS-–æ—Ü–µ–Ω–∫–∏)

# === –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –≤–æ–ø—Ä–æ—Å–∞ ===

def is_prompt_relevant_to_documents(relevance_scores, relevance_threshold=RELEVANCE_THRESHOLD_PROMPT) -> bool:
    try:
        if not relevance_scores:
            return False

        max_similarity = max((sim for _, sim in relevance_scores), default=0.0)
        logger.info(f"üîç –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ FAISS: {max_similarity:.4f}, –ø–æ—Ä–æ–≥: {relevance_threshold}")
        return max_similarity >= relevance_threshold
    except Exception as e:
        logger.error(f"Exception in is_prompt_relevant_to_documents: {str(e)}")
        return False

# === –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ç–≤–µ—Ç–∞ LLM ===

async def postprocess_llm_response(
        llm_response: str,
        user_prompt: str,
        context_str: str = "",
        references: dict = None,
        is_relevant: bool = False,
        model_client: ModelClient = None
) -> tuple:
    if references is None:
        references = {}

    prompt_references = (
        "You are an advanced language model tasked with providing a final, "
        "well-structured answer based on the given content.\n\n"
        "### Provided Data\n"
        f"LLM raw response:\n{llm_response}\n\n"
        f"User prompt:\n{user_prompt}\n\n"
        f"Context:\n{context_str}\n\n"
        f"References:\n{references}\n\n"
        f"is_relevant: {is_relevant}\n"
        "-------------------------\n"
        "Please re-check clarity and, if references exist, list them at the end.\n"
        "Return the final improved answer now:\n"
    )

    final_answer = await model_client.generate_text(prompt_references)
    return final_answer, references

# === –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º ===

async def generate_response(
        prompt: str,
        model_client: ModelClient,
        chat_history=None,
        metadata_filters=None,
        context=None
):
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ embeddings
    if not model_client.embeddings_available:
        print("‚ùå Embeddings –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã. RAG —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –Ω–µ –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å.")
        fallback_answer = "Embeddings –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É—é –±–∞–∑–æ–≤—ã–π –æ—Ç–≤–µ—Ç."
        return fallback_answer, None

    # 1. –ó–∞–≥—Ä—É–∑–∫–∞/—Å–æ–∑–¥–∞–Ω–∏–µ vector_store
    success = load_and_index_documents(KNOWLEDGE_BASE_FILE, VECTOR_STORE_DIR, model_client.embeddings)
    if not success:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏ –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã.")
        return "Unable to load Vector Store.", None

    vector_store = load_vector_store(VECTOR_STORE_DIR, model_client.embeddings)
    logger.info(f"vector_store = {vector_store}")
    if not vector_store:
        return "Unable to load Vector Store.", None

    # 2. –ü—Ä–µ–¥–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤–æ–ø—Ä–æ—Å
    if chat_history is None:
        chat_history = []
    prepared_prompt = await preprocess_user_prompt(prompt, chat_history, model_client)

    # 3. –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ FAISS
    retrieved_docs_with_scores = retrieve_documents(
        vector_store=vector_store,
        user_prompt=prepared_prompt,
        k=DOCS_IN_RETRIEVER,
        metadata_filters=metadata_filters
    )

    # ----------
    logger.info(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(retrieved_docs_with_scores)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ FAISS")

    for i, (doc, score) in enumerate(retrieved_docs_with_scores[:3]):
        logger.info(f"üìÑ –¢–æ–ø-{i + 1} –¥–æ–∫—É–º–µ–Ω—Ç (–æ—Ü–µ–Ω–∫–∞={score:.3f}): {doc.page_content[:200]}...")

    # ----------

    # 4. –ò—Å–ø–æ–ª—å–∑—É–µ–º FAISS-–æ—Ü–µ–Ω–∫–∏, –ù–ï –ø–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ–º
    relevance_scores = retrieved_docs_with_scores  # [(doc, score), ...]

    # 5. –§–∏–ª—å—Ç—Ä—É–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ RELEVANCE_THRESHOLD_DOCS
    relevant_docs = [
        doc for (doc, similarity) in relevance_scores
        if similarity >= RELEVANCE_THRESHOLD_DOCS
    ]

    # ----------
    # –ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
    logger.info(f"‚úÖ –û—Å—Ç–∞–ª–æ—Å—å {len(relevant_docs)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏")

    # ----------

    # 6. –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞—à–ª–æ—Å—å, –±–µ—Ä—ë–º –ª—É—á—à–∏–π
    if not relevant_docs and relevance_scores:
        # –í–æ–∑—å–º–∏ —Ö–æ—Ç—è –±—ã —Å–∞–º—ã–π –ª—É—á—à–∏–π, –¥–∞–∂–µ –µ—Å–ª–∏ < 0.5
        best_doc, best_score = max(relevance_scores, key=lambda x: x[1])
        logger.warning(f"‚ö†Ô∏è –ù–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤—ã—à–µ –ø–æ—Ä–æ–≥–∞, –Ω–æ –±–µ—Ä—ë–º –ª—É—á—à–∏–π (score={best_score:.3f})")
        relevant_docs = [best_doc]

    # 7. –§–æ—Ä–º–∏—Ä—É–µ–º ¬´–∫–æ–Ω—Ç–µ–∫—Å—Ç¬ª –∏–∑ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    context_str = ""
    for doc in relevant_docs:
        source = doc.metadata.get('source', 'Unknown')
        page = doc.metadata.get('page', 'N/A')
        content = doc.page_content or 'N/A'
        context_str += f"Source: {source}, Page: {page}\nContent:\n{content}\n---\n"

    # 8. ¬´–°–∏—Å—Ç–µ–º–Ω—ã–π¬ª –ø—Ä–æ–º–ø—Ç: –¥–∞—ë–º –º–æ–¥–µ–ª–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç
    system_prompt = (
        "You are an expert. Provide a concise answer based on the context:\n"
        f"{context_str}\n"
        "--- End Context ---\n"
        "If the user question isn't fully answered in the provided context, "
        "use your best judgment while staying truthful.\n"
    )

    # 9. –§–æ—Ä–º–∏—Ä—É–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è LLM
    final_prompt = f"{system_prompt}\n\nUser: {prepared_prompt}"

    # 10. –í—ã–∑—ã–≤–∞–µ–º LLM
    answer_text = await model_client.generate_text(final_prompt)

    # 11. –û—Ü–µ–Ω–∏–≤–∞–µ–º ¬´–≥–ª–æ–±–∞–ª—å–Ω—É—é¬ª —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
    is_relevant = is_prompt_relevant_to_documents(relevance_scores)

    # 12. –ì–æ—Ç–æ–≤–∏–º —Å–ø–∏—Å–æ–∫ —Å—Å—ã–ª–æ–∫
    references = {}
    for doc in relevant_docs:
        filename = doc.metadata.get("source", "Unknown")
        page = doc.metadata.get("page", "N/A")
        references.setdefault(filename, set()).add(page)

    # 13. –ü–æ—Å—Ç-–æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ç–≤–µ—Ç–∞
    final_answer, processed_refs = await postprocess_llm_response(
        llm_response=answer_text,
        user_prompt=prompt,
        context_str=context_str,
        references=references,
        is_relevant=is_relevant,
        model_client=model_client
    )

    # 14. –ò—Ç–æ–≥–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
    if is_relevant:
        final_text = final_answer + "\n---\nAdditional references may be listed above."
        source_files = list(processed_refs.keys()) if processed_refs else None
    else:
        final_text = final_answer
        source_files = None

    return final_text, source_files

# === —Ç–µ—Å—Ç ===

async def interactive_demo():
    config = ModelConfig()
    model_client = ModelClient(config)

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ embeddings
    if not model_client.embeddings_available:
        print("‚ö†Ô∏è Embeddings –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã. –ü—Ä–æ–≤–µ—Ä—å URL –∏–ª–∏ –º–æ–¥–µ–ª—å.")
        print("üí° –ë—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω –∫–∞—Å—Ç–æ–º–Ω—ã–π –ø–æ–∏—Å–∫ (–±–µ–∑ –≤–µ–∫—Ç–æ—Ä–æ–≤).")

    print("\n‚úÖ –ê—Å—Å–∏—Å—Ç–µ–Ω—Ç –≥–æ—Ç–æ–≤. –ó–∞–¥–∞–≤–∞–π –≤–æ–ø—Ä–æ—Å—ã. –í–≤–µ–¥–∏ 'exit' –¥–ª—è –≤—ã—Ö–æ–¥–∞.\n")

    chat_history = []

    while True:
        question = input("–í–æ–ø—Ä–æ—Å: ").strip()
        if question.lower() in ['exit', 'quit', '–≤—ã–π—Ç–∏', 'q']:
            print("üëã –í—ã—Ö–æ–¥.")
            break

        if not question:
            continue

        try:
            answer, sources = await generate_response(
                prompt=question,
                model_client=model_client,
                chat_history=chat_history
            )
            print(f"ü§ñ –û—Ç–≤–µ—Ç: {answer}\n")
            if sources:
                print(f"üìö –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏: {sources}\n")

            # –ü—Ä–∞–≤–∏–ª—å–Ω–æ–µ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ chat_history
            chat_history.append(HumanMessage(content=question))
            chat_history.append(AIMessage(content=answer))
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}")

if __name__ == "__main__":
    asyncio.run(interactive_demo())