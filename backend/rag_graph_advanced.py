# rag_graph_advanced.py
import logging
import os
import pickle
from typing import List, Dict, Any, TypedDict, Optional
from datetime import datetime

import numpy as np
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import Docx2txtLoader, PyPDFLoader, TextLoader
from langchain_community.document_loaders import UnstructuredExcelLoader
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from pydantic import BaseModel
from dotenv import load_dotenv
from langgraph.graph import StateGraph, END
import redis

# === –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è ===
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(name)s: %(message)s",
    handlers=[
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# === –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ (–¥–ª—è FastAPI) ===
VECTOR_STORES: Dict[str, FAISS] = {}
DOCUMENT_MAP: Dict[str, Any] = {}

# === –ö—ç—à: –Ω–∞ –¥–∏—Å–∫–µ –∏–ª–∏ Redis ===
class CacheManager:
    def __init__(self, cache_dir: str = "data/cache"):
        self.cache_dir = cache_dir
        os.makedirs(self.cache_dir, exist_ok=True)
        self.cache_path = os.path.join(self.cache_dir, "answers.pkl")
        self.cache = {}
        self.load_cache()

    def load_cache(self):
        if os.path.exists(self.cache_path):
            try:
                with open(self.cache_path, "rb") as f:
                    self.cache = pickle.load(f)
                logger.info(f"üíæ –ö—ç—à –∑–∞–≥—Ä—É–∂–µ–Ω: {len(self.cache)} –∑–∞–ø–∏—Å–µ–π")
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫—ç—à–∞: {e}")

    def save_cache(self):
        try:
            with open(self.cache_path, "wb") as f:
                pickle.dump(self.cache, f)
            logger.info(f"üíæ –ö—ç—à —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {len(self.cache)} –∑–∞–ø–∏—Å–µ–π")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫—ç—à–∞: {e}")

    def get(self, key: str) -> Optional[str]:
        return self.cache.get(key)

    def set(self, key: str, value: str, ttl=3600):
        self.cache[key] = value
        self.save_cache()

cache_manager = CacheManager()

# === –ú–æ–¥–µ–ª—å –¥–ª—è –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏ ===

class FileSelection(BaseModel):
    filename: str
    reason: str

# === –ö–ª–∞—Å—Å ModelClient ===

class ModelClient:
    def __init__(
        self,
        embedding_base_url: str,
        generative_base_url: str,
        embedding_model: str,
        generative_model: str
    ):
        self.llm = ChatOpenAI(
            api_key="not-needed",
            base_url=generative_base_url,
            model=generative_model,
            temperature=0.6,
        )
        try:
            self.embeddings = OpenAIEmbeddings(
                api_key="not-needed",
                base_url=embedding_base_url,
                model=embedding_model,
            )
            logger.info("‚úÖ Embeddings —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã.")
            self.embeddings_available = True
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ embeddings: {e}")
            self.embeddings_available = False

    async def agenerate(self, prompt: str, system_prompt: str = None) -> str:
        messages = []
        if system_prompt:
            messages.append(("system", system_prompt))
        messages.append(("user", prompt))
        response = await self.llm.ainvoke(messages)
        return response.content

# === –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π .xlsx ===

def get_loader(file_path: str):
    ext = file_path.lower().split(".")[-1]
    if ext == "docx":
        return Docx2txtLoader(file_path)
    elif ext == "pdf":
        return PyPDFLoader(file_path)
    elif ext == "txt" or ext == "md":
        return TextLoader(file_path, encoding="utf-8")
    elif ext == "xlsx" or ext == "xls":
        return UnstructuredExcelLoader(file_path, mode="elements")
    else:
        raise ValueError(f"–§–æ—Ä–º–∞—Ç {ext} –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è")

async def load_and_index_all_documents(
    documents_dir: str = "data/documents",
    vector_store_dir: str = "data/vector_stores",
    embedding_base_url: str = "http://model-embedding.shared.du.iba/v1",
    embedding_model: str = "embedding-model",
    batch_size: int = 1,
    chunk_size: int = 500,
    chunk_overlap: int = 100
):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏ —Å–æ–∑–¥–∞—ë—Ç –æ—Ç–¥–µ–ª—å–Ω—ã–π vector store –¥–ª—è –∫–∞–∂–¥–æ–≥–æ"""
    global VECTOR_STORES

    model_client = ModelClient(
        embedding_base_url=embedding_base_url,
        generative_base_url="http://model-generative.shared.du.iba/v1",
        embedding_model=embedding_model,
        generative_model="generative-model"
    )

    if not model_client.embeddings_available:
        raise RuntimeError("Embeddings –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã")

    os.makedirs(vector_store_dir, exist_ok=True)

    for filename in os.listdir(documents_dir):
        file_path = os.path.join(documents_dir, filename)
        if not os.path.isfile(file_path):
            continue

        store_dir = os.path.join(vector_store_dir, os.path.splitext(filename)[0])
        os.makedirs(store_dir, exist_ok=True)

        index_file = os.path.join(store_dir, "index.faiss")
        if os.path.exists(index_file):
            try:
                vector_store = FAISS.load_local(
                    store_dir,
                    model_client.embeddings,
                    allow_dangerous_deserialization=True
                )
                VECTOR_STORES[filename] = vector_store
                logger.info(f"‚úÖ –í–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –∑–∞–≥—Ä—É–∂–µ–Ω–æ: {filename}")
                continue
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å {filename}, –ø–µ—Ä–µ—Å–æ–∑–¥–∞—ë–º: {e}")

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏ –∏–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º
        try:
            loader = get_loader(file_path)
            docs = loader.load()

            # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–∞–±–ª–∏—Ü...
            if any("table" in doc.metadata.get("type", "").lower() for doc in docs):
                logger.info(f"üìÑ –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã —Ç–∞–±–ª–∏—Ü—ã –≤ {filename}, –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Ç–µ–∫—Å—Ç...")
                cleaned_docs = []
                for doc in docs:
                    if doc.metadata.get("type") == "table":
                        table_text = doc.page_content.replace("\n", " | ").replace("  ", " ")
                        cleaned_docs.append(
                            type(doc)(
                                page_content=f"–¢–∞–±–ª–∏—Ü–∞ –∏–∑ {filename}:\n{table_text}",
                                metadata={**doc.metadata, "source": filename, "type": "table"}
                            )
                        )
                    else:
                        cleaned_docs.append(doc)
                docs = cleaned_docs

            # –ò–°–ü–û–õ–¨–ó–£–ï–ú –ü–ê–†–ê–ú–ï–¢–†–´
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=chunk_size,
                chunk_overlap=chunk_overlap
            )
            split_docs = text_splitter.split_documents(docs)

            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ –±–∞—Ç—á–∞–º —Å –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–º batch_size
            vector_store = None
            for i in range(0, len(split_docs), batch_size):
                batch = split_docs[i:i + batch_size]
                try:
                    if vector_store is None:
                        vector_store = FAISS.from_documents(batch, model_client.embeddings)
                    else:
                        vector_store.add_documents(batch)
                    # logger.info(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω –±–∞—Ç—á {i//batch_size + 1} –¥–ª—è {filename}")
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ –±–∞—Ç—á–µ {i//batch_size + 1} –¥–ª—è {filename}: {e}")
                    continue

            if vector_store is not None:
                vector_store.save_local(store_dir)
                VECTOR_STORES[filename] = vector_store
                logger.info(f"‚úÖ –î–æ–∫—É–º–µ–Ω—Ç –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω: {filename}")
            else:
                logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å {filename}")

        except Exception as e:
            logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ {filename}: {e}")
            continue

# === –ú–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è: –≤—ã–±—Ä–∞—Ç—å —Ñ–∞–π–ª ===

async def route_question_to_file(
    question: str,
    chat_history: List,
    embedding_base_url: str = "http://model-embedding.shared.du.iba/v1",
    generative_base_url: str = "http://model-generative.shared.du.iba/v1"
) -> str:
    model_client = ModelClient(
        embedding_base_url=embedding_base_url,
        generative_base_url=generative_base_url,
        embedding_model="embedding-model",
        generative_model="generative-model"
    )

    files_list = list(VECTOR_STORES.keys())
    if not files_list:
        return "–°–≠–î.docx"

    prompt = f"""
–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏ –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ —Å–∏—Å—Ç–µ–º–µ —ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–æ–±–æ—Ä–æ—Ç–∞.
–î–æ—Å—Ç—É–ø–Ω—ã–µ —Ñ–∞–π–ª—ã: {files_list}

–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Å–ø—Ä–∞—à–∏–≤–∞–µ—Ç: "{question}"

–ò—Å—Ç–æ—Ä–∏—è —á–∞—Ç–∞: {chat_history}

–í—ã–±–µ—Ä–∏ –û–î–ò–ù —Ñ–∞–π–ª, –≤ –∫–æ—Ç–æ—Ä–æ–º, —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ, –µ—Å—Ç—å –æ—Ç–≤–µ—Ç.
–û—Ç–≤–µ—Ç—å –¢–û–õ–¨–ö–û –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON:
{{
  "filename": "–∏–º—è_—Ñ–∞–π–ª–∞",
  "reason": "–∫—Ä–∞—Ç–∫–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ, –ø–æ—á–µ–º—É –∏–º–µ–Ω–Ω–æ —ç—Ç–æ—Ç —Ñ–∞–π–ª"
}}
"""

    try:
        response = await model_client.agenerate(prompt)
        import json
        result = json.loads(response)
        filename = result.get("filename", "").strip()
        if filename in VECTOR_STORES:
            logger.info(f"üîç –ú–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è: '{question}' ‚Üí {filename} ({result.get('reason', '–Ω–µ —É–∫–∞–∑–∞–Ω–æ')})")
            return filename
        else:
            logger.warning(f"‚ö†Ô∏è –í—ã–±—Ä–∞–Ω –Ω–µ—Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ñ–∞–π–ª: {filename}, –∏—Å–ø–æ–ª—å–∑—É–µ–º –°–≠–î.docx")
            return "–°–≠–î.docx"
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏: {e}")
        return "–°–≠–î.docx"

# === RAG: –ø–æ–∏—Å–∫ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è ===

async def retrieve_and_generate(
    question: str,
    filename: str,
    chat_history: List,
    embedding_base_url: str = "http://model-embedding.shared.du.iba/v1",
    generative_base_url: str = "http://model-generative.shared.du.iba/v1"
) -> str:
    vector_store = VECTOR_STORES.get(filename)
    if not vector_store:
        return "‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω."

    # –ü–æ–∏—Å–∫
    docs_with_scores = vector_store.similarity_search_with_score(question, k=5)
    relevant_docs = [doc for doc, score in docs_with_scores if score >= 0.5]

    if not relevant_docs and docs_with_scores:
        best_doc, _ = docs_with_scores[0]
        relevant_docs = [best_doc]

    if not relevant_docs:
        return "–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ —ç—Ç–æ–º —Ñ–∞–π–ª–µ."

    context = "\n\n".join([
        f"–ò—Å—Ç–æ—á–Ω–∏–∫: {doc.metadata.get('source', 'Unknown')}\n"
        f"–¢–∏–ø: {doc.metadata.get('type', 'text')}\n"
        f"–°–æ–¥–µ—Ä–∂–∏–º–æ–µ:\n{doc.page_content}"
        for doc in relevant_docs
    ])

    system_prompt = f"""–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –°–≠–î. –û—Ç–≤–µ—á–∞–π —Ç–æ–ª—å–∫–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.
–ï—Å–ª–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –æ—Ç–≤–µ—Ç ‚Äî —Å–∫–∞–∂–∏ ¬´–Ø –Ω–µ –Ω–∞—à—ë–ª –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö¬ª.
–ù–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π –Ω–∏—á–µ–≥–æ –æ—Ç —Å–µ–±—è.

–ö–æ–Ω—Ç–µ–∫—Å—Ç:
{context}
"""

    prompt = f"–í–æ–ø—Ä–æ—Å: {question}\n–ò—Å—Ç–æ—Ä–∏—è: {chat_history}\n–û—Ç–≤–µ—Ç:"
    model_client = ModelClient(
        embedding_base_url=embedding_base_url,
        generative_base_url=generative_base_url,
        embedding_model="embedding-model",
        generative_model="generative-model"
    )
    answer = await model_client.agenerate(prompt, system_prompt=system_prompt)

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ "–ø—É—Å—Ç–æ–π" –∏–ª–∏ "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π" –æ—Ç–≤–µ—Ç
    if any(phrase in answer.lower() for phrase in [
        "—è –Ω–µ –Ω–∞—à—ë–ª", "–Ω–µ –º–æ–≥—É –Ω–∞–π—Ç–∏", "–Ω–µ —É–∫–∞–∑–∞–Ω–æ", "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ",
        "–Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç—Å—è", "–Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏", "–Ω–µ –Ω–∞—à—ë–ª", "–Ω–µ –º–æ–≥—É –æ—Ç–≤–µ—Ç–∏—Ç—å"
    ]):
        return "REFLECT: –ù–µ –Ω–∞–π–¥–µ–Ω–æ –≤ —ç—Ç–æ–º —Ñ–∞–π–ª–µ"

    return answer

# === –†–µ—Ñ–ª–µ–∫—Å–∏—è: –µ—Å–ª–∏ –æ—Ç–≤–µ—Ç –ø–ª–æ—Ö–æ–π ‚Äî –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –¥—Ä—É–≥–æ–π —Ñ–∞–π–ª ===

async def reflect_and_retry(
    question: str,
    chat_history: List,
    initial_file: str,
    embedding_base_url: str = "http://model-embedding.shared.du.iba/v1",
    generative_base_url: str = "http://model-generative.shared.du.iba/v1"
) -> str:
    """–ï—Å–ª–∏ –æ—Ç–≤–µ—Ç –ø–ª–æ—Ö–æ–π ‚Äî –ø–æ–ø—Ä–æ–±—É–µ–º –¥—Ä—É–≥–∏–µ —Ñ–∞–π–ª—ã"""
    model_client = ModelClient(
        embedding_base_url=embedding_base_url,
        generative_base_url=generative_base_url,
        embedding_model="embedding-model",
        generative_model="generative-model"
    )

    other_files = [f for f in VECTOR_STORES.keys() if f != initial_file]
    if not other_files:
        return "–ù–µ—Ç –¥—Ä—É–≥–∏—Ö —Ñ–∞–π–ª–æ–≤ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏."

    logger.info(f"üîÑ –†–µ—Ñ–ª–µ–∫—Å–∏—è: –æ—Ç–≤–µ—Ç –ø–ª–æ—Ö–æ–π, –ø—Ä–æ–±—É–µ–º –¥—Ä—É–≥–∏–µ —Ñ–∞–π–ª—ã: {other_files}")

    for alt_file in other_files:
        logger.info(f"üîÅ –ü–æ–ø—ã—Ç–∫–∞ –≤ —Ñ–∞–π–ª–µ: {alt_file}")
        answer = await retrieve_and_generate(question, alt_file, chat_history, embedding_base_url, generative_base_url)
        if answer != "REFLECT: –ù–µ –Ω–∞–π–¥–µ–Ω–æ –≤ —ç—Ç–æ–º —Ñ–∞–π–ª–µ" and len(answer.strip()) > 20:
            logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –Ω–∞–π–¥–µ–Ω–æ –≤ —Ñ–∞–π–ª–µ: {alt_file}")
            return answer

    logger.info("‚ùå –†–µ—Ñ–ª–µ–∫—Å–∏—è –∑–∞–≤–µ—Ä—à–∏–ª–∞—Å—å –Ω–µ—É–¥–∞—á–Ω–æ ‚Äî –≤–æ–∑–≤—Ä–∞—Ç –∫ –ø–µ—Ä–≤–æ–Ω–∞—á–∞–ª—å–Ω–æ–º—É –æ—Ç–≤–µ—Ç—É")
    return "–Ø –Ω–µ –Ω–∞—à—ë–ª –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö."

# === LangGraph State ===

class AgentState(TypedDict):
    question: str
    chat_history: List
    selected_file: str
    answer: str
    retry_count: int

# === Nodes ===

async def decide_file_node(state: AgentState) -> AgentState:
    filename = await route_question_to_file(
        state["question"],
        state["chat_history"],
        embedding_base_url=os.getenv("EMBEDDING_BASE_URL", "http://model-embedding.shared.du.iba/v1"),
        generative_base_url=os.getenv("GENERATIVE_BASE_URL", "http://model-generative.shared.du.iba/v1")
    )
    return {**state, "selected_file": filename}

async def retrieve_node(state: AgentState) -> AgentState:
    answer = await retrieve_and_generate(
        state["question"],
        state["selected_file"],
        state["chat_history"],
        embedding_base_url=os.getenv("EMBEDDING_BASE_URL", "http://model-embedding.shared.du.iba/v1"),
        generative_base_url=os.getenv("GENERATIVE_BASE_URL", "http://model-generative.shared.du.iba/v1")
    )
    return {**state, "answer": answer}

async def reflect_node(state: AgentState) -> AgentState:
    if state["answer"] == "REFLECT: –ù–µ –Ω–∞–π–¥–µ–Ω–æ –≤ —ç—Ç–æ–º —Ñ–∞–π–ª–µ" and state["retry_count"] < 2:
        logger.info(f"üîÑ –ó–∞–ø—É—Å–∫ —Ä–µ—Ñ–ª–µ–∫—Å–∏–∏ (–ø–æ–ø—ã—Ç–∫–∞ {state['retry_count'] + 1}/2)")
        answer = await reflect_and_retry(
            state["question"],
            state["chat_history"],
            state["selected_file"],
            embedding_base_url=os.getenv("EMBEDDING_BASE_URL", "http://model-embedding.shared.du.iba/v1"),
            generative_base_url=os.getenv("GENERATIVE_BASE_URL", "http://model-generative.shared.du.iba/v1")
        )
        return {**state, "answer": answer, "retry_count": state["retry_count"] + 1}
    else:
        return state

# === Conditional Edge: –µ—Å–ª–∏ –æ—Ç–≤–µ—Ç –ø–ª–æ—Ö–æ–π ‚Üí —Ä–µ—Ñ–ª–µ–∫—Å–∏—è ===

def should_reflect(state: AgentState) -> str:
    if state["answer"] == "REFLECT: –ù–µ –Ω–∞–π–¥–µ–Ω–æ –≤ —ç—Ç–æ–º —Ñ–∞–π–ª–µ" and state["retry_count"] < 2:
        return "reflect"
    else:
        return "end"

# === Graph ===

def create_rag_graph():
    workflow = StateGraph(AgentState)

    workflow.add_node("decide_file", decide_file_node)
    workflow.add_node("retrieve", retrieve_node)
    workflow.add_node("reflect", reflect_node)

    workflow.set_entry_point("decide_file")
    workflow.add_edge("decide_file", "retrieve")
    workflow.add_conditional_edges(
        "retrieve",
        should_reflect,
        {
            "reflect": "reflect",
            "end": END
        }
    )
    workflow.add_edge("reflect", END)

    return workflow.compile()

# === –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è (–¥–ª—è FastAPI) ===

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(name)s: %(message)s",
    handlers=[
        logging.StreamHandler()
    ]
)
# logger = logging.getLogger(__name__)